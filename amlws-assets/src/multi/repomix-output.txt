This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-15T00:29:29.972Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
endpoints/
  batch/
    batch-deployment.yml
    batch-endpoint.yml
  online/
    online-deployment.yml
    online-endpoint.yml
environment/
  automl-conda.yml
  train-conda.yml
pipelinejobs/
  eegpatientrai.py
  eegpatienttrain.py
  eegpreprocess.py
  eegwindowrai.py
  eegwindowtrain.py
src/
  multi/
    data_loader.py
    downsampler.py
    extract_features.py
    filter.py
    register_features.py
    train_from_agg_features.py
    train_from_features.py
    upsampler.py
    window_slicer.py
  split_data_patient.py
  split_data_window.py

================================================================
Repository Files
================================================================

================
File: endpoints/batch/batch-deployment.yml
================
$schema: https://azuremlschemas.azureedge.net/latest/batchDeployment.schema.json
name: batch-dp
endpoint_name: $(Build.Repository.Name)-batch
model: azureml:automl-model@latest
compute: azureml:batch-cluster
resources:
  instance_count: 1
max_concurrency_per_instance: 2
mini_batch_size: 10
output_action: append_row
output_file_name: predictions.csv
retry_settings:
  max_retries: 3
  timeout: 30
error_threshold: -1
logging_level: info

================
File: endpoints/batch/batch-endpoint.yml
================
$schema: https://azuremlschemas.azureedge.net/latest/batchEndpoint.schema.json
name: $(Build.Repository.Name)-batch
description: Model endpoint
auth_mode: aad_token

================
File: endpoints/online/online-deployment.yml
================
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: $(Build.Repository.Name)-online
model: azureml:automl-model@latest
instance_type: Standard_DS3_v2
instance_count: 1

================
File: endpoints/online/online-endpoint.yml
================
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json
name: $(Build.Repository.Name)-online
description: Model endpoint
auth_mode: key

================
File: environment/automl-conda.yml
================
name: automl
channels:
  - conda-forge
dependencies:
  - python=3.10
  - pip
  - pip:
    - azureml-sdk==1.57.0
    - azureml-train-automl-runtime==1.57.0
    - azureml-widgets==1.57.0
    - azureml-mlflow==1.57.0
    - azure-graphrbac==0.61.1
    - azure-ai-ml==1.12.1
    - pyarrow==14.0.2
    - pandas==1.3.5
    - numpy~=1.22.4
    - joblib==1.2.0
    - matplotlib~=3.7.1
    - azure-core~=1.26.4
    - azure-identity~=1.7.0
    - azure-storage-queue~=12.6.0
    - opentelemetry-api~=1.20.0
    - opentelemetry-sdk~=1.20.0
    - argcomplete<=3.4.0
    - attrs<=23.2.0
    - cachetools<=5.4.0
    - certifi<=2024.7.4
    - cffi<=1.16.0
    - databricks-sdk<=0.29.0
    - dill<=0.3.8
    - fire<=0.6.0
    - fsspec<=2024.6.1
    - google-auth<=2.32.0
    - jsonpickle<=3.2.2
    - mlflow-skinny<=2.15.0
    - msal<=1.30.0
    - msgpack<=1.0.8
    - paramiko<=3.4.0
    - pyasn1<=0.6.0
    - rpds-py<=0.19.1
    - scikit-learn<=1.5.1
    - sympy<=1.13.1
    - urllib3<=1.26.19
    - zipp<=3.19.2
    - pytz<=2024.1
    - charset-normalizer==3.3.2
    - pillow==10.4.0
    - pkginfo==1.11.1
    - scipy==1.10.0
    - nolds==0.6.1
    - imbalanced-learn
    - >-
      git+https://github.com/microsoft/AzureML-Observability#subdirectory=aml-obs-client
    - >-
      git+https://github.com/microsoft/AzureML-Observability#subdirectory=aml-obs-collector
    - isodate<=0.6.1
    - jsonschema-specifications<=2023.12.1
    - termcolor<=2.4.0
    - toolz<=0.12.1

================
File: environment/train-conda.yml
================
channels:
  - defaults
  - conda-forge
dependencies:
  - python=3.10
  - pip
  - pip:
    - azureml-mlflow
    - azure-ai-ml
    - pyarrow
    - scikit-learn
    - pandas
    - joblib
    - matplotlib
    - git+https://github.com/microsoft/AzureML-Observability#subdirectory=aml-obs-client
    - git+https://github.com/microsoft/AzureML-Observability#subdirectory=aml-obs-collector

================
File: pipelinejobs/eegpatientrai.py
================
import argparse
from azure.ai.ml.entities import Data, Model
from azure.ai.ml.constants import AssetTypes
from azure.identity import ClientSecretCredential
from azure.ai.ml import MLClient, Input, Output, command, dsl
import os
import json
import uuid
import time
import logging
from typing import Dict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("Deploy EEG Analysis Pipeline")
    parser.add_argument("--experiment_name", type=str, required=True, help="Experiment Name")
    parser.add_argument("--compute_name", type=str, required=True, help="Compute Cluster Name")
    parser.add_argument("--data_name", type=str, required=True, help="Data Asset Name")
    parser.add_argument("--model_name", type=str, required=True, help="Model Name")
    parser.add_argument("--version", type=str, required=True, help="Version of registered features")
    parser.add_argument("--model_version", type=str, required=False, default="latest", help="Version of the registered model")
    parser.add_argument("--environment_name", type=str, required=True, help="Environment name")
    return parser.parse_args()

def setup_rai_components(ml_client_registry):
    """Set up RAI components from registry
    
    Args:
        ml_client_registry: Azure ML registry client
        
    Returns:
        dict: Dictionary containing RAI components
    """
    logger.info("Setting up RAI components...")
    label = "latest"
    
    rai_constructor = ml_client_registry.components.get(
        name="microsoft_azureml_rai_tabular_insight_constructor", 
        label=label
    )
    version = rai_constructor.version
    logger.info(f"Using RAI components version: {version}")
    
    components = {
        'constructor': rai_constructor,
        'error_analysis': ml_client_registry.components.get(
            name="microsoft_azureml_rai_tabular_erroranalysis", 
            version=version
        ),
        'explanation': ml_client_registry.components.get(
            name="microsoft_azureml_rai_tabular_explanation", 
            version=version
        ),
        'gather': ml_client_registry.components.get(
            name="microsoft_azureml_rai_tabular_insight_gather", 
            version=version
        )
    }
    logger.info("RAI components setup complete")
    return components

def create_split_component(environment_name):
    """Create the data splitting component"""
    return command(
        name="split_data",
        display_name="split-data",
        code="amlws-assets/src",
        command="python split_data_patient.py \
                --input_mltable ${{inputs.input_mltable}} \
                --train_data ${{outputs.train_data}} \
                --test_data ${{outputs.test_data}} \
                --test_size 0.4",
        environment=environment_name+"@latest",
        inputs={
            "input_mltable": Input(type="mltable")
        },
        outputs={
            "train_data": Output(type="mltable"),
            "test_data": Output(type="mltable")
        }
    )

def create_rai_pipeline(
    compute_name: str,
    model_name: str,
    model_version: str,
    target_column_name: str,
    input_data: Input,
    environment_name: str,
    rai_components: Dict
):
    """Create the RAI pipeline with all components"""
    # Create the split component
    split_data = create_split_component(environment_name)
    
    @dsl.pipeline(
        compute=compute_name,
        description="RAI insights on EEG data",
        experiment_name=f"RAI_insights_{model_name}",
    )
    def rai_decision_pipeline(
        target_column_name, input_data
    ):
        # Split the data
        split_job = split_data(
            input_mltable=input_data
        )
        
        expected_model_id = f"{model_name}:{model_version}"
        azureml_model_id = f"azureml:{expected_model_id}"
        
        create_rai_job = rai_components['constructor'](
            title="RAI dashboard EEG",
            task_type="classification",
            model_info=expected_model_id,
            model_input=Input(type=AssetTypes.MLFLOW_MODEL, path=azureml_model_id),
            train_dataset=split_job.outputs.train_data,
            test_dataset=split_job.outputs.test_data,
            target_column_name=target_column_name,
            classes='[false, true]'
        )
        create_rai_job.set_limits(timeout=300)

        error_job = rai_components['error_analysis'](
            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,
        )
        error_job.set_limits(timeout=300)

        explanation_job = rai_components['explanation'](
            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,
            comment="add explanation", 
        )
        explanation_job.set_limits(timeout=300)

        gather_job = rai_components['gather'](
            constructor=create_rai_job.outputs.rai_insights_dashboard,
            insight_3=error_job.outputs.error_analysis,
            insight_4=explanation_job.outputs.explanation,
        )
        gather_job.set_limits(timeout=300)

        gather_job.outputs.dashboard.mode = "upload"

        return {
            "dashboard": gather_job.outputs.dashboard,
        }
    
    return rai_decision_pipeline(
        target_column_name=target_column_name,
        input_data=input_data
    )

def main():
    try:
        logger.info("Starting RAI pipeline deployment")
        args = parse_args()
        
        # Set up Azure ML client
        logger.info("Setting up Azure ML client")
        credential = ClientSecretCredential(
            client_id=os.environ["AZURE_CLIENT_ID"],
            client_secret=os.environ["AZURE_CLIENT_SECRET"],
            tenant_id=os.environ["AZURE_TENANT_ID"]
        )
        ml_client = MLClient.from_config(credential=credential)
        
        # Verify compute cluster
        compute_target = ml_client.compute.get(args.compute_name)
        logger.info(f"Found compute target: {compute_target.name}")
        
        # Verify model exists
        model = ml_client.models.get(args.model_name + "-patient", version=args.model_version)
        logger.info(f"Found model {args.model_name + '-patient'} (version {model.version}) with URI: {model.path}")
        
        # Get RAI components from registry
        registry_name = "azureml"
        logger.info(f"Accessing registry: {registry_name}")
        ml_client_registry = MLClient(
            credential=credential,
            subscription_id=ml_client.subscription_id,
            resource_group_name=ml_client.resource_group_name,
            registry_name=registry_name,
        )
        
        # Setup RAI components
        rai_components = setup_rai_components(ml_client_registry)
        
        # Get the registered MLTable
        logger.info(f"Getting registered features version: {args.version}")
        registered_features = Input(
            type="mltable",
            path=f"azureml:{args.data_name}:{args.version}",
            mode="ro_mount"
        )
        
        # Create pipeline
        logger.info("Creating RAI pipeline job")
        pipeline_job = create_rai_pipeline(
            compute_name=args.compute_name,
            model_name=args.model_name + "-patient",
            model_version=args.model_version,
            target_column_name="Remission",
            input_data=registered_features,
            environment_name=args.environment_name,
            rai_components=rai_components
        )
        
        # Configure pipeline settings
        logger.info("Configuring pipeline settings")
        pipeline_job.settings.default_compute = args.compute_name
        pipeline_job.settings.default_datastore = "workspaceblobstore"
        pipeline_job.settings.continue_on_step_failure = False
        pipeline_job.settings.force_rerun = True
        pipeline_job.settings.default_timeout = 3600
        
        # Submit and monitor pipeline job
        logger.info(f"Submitting pipeline job to experiment: {args.experiment_name}")
        pipeline_job = ml_client.jobs.create_or_update(
            pipeline_job, experiment_name=args.experiment_name
        )
        
        logger.info(f"Pipeline job submitted. Job name: {pipeline_job.name}")
        logger.info("Starting job monitoring...")
        ml_client.jobs.stream(pipeline_job.name)
        
    except Exception as e:
        logger.error(f"Pipeline deployment failed: {str(e)}")
        raise

if __name__ == "__main__":
    main()

================
File: pipelinejobs/eegpatienttrain.py
================
import argparse
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import ClientSecretCredential
from azure.ai.ml import MLClient, Input, Output, command, dsl
import os
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("Deploy EEG Training Pipeline")
    parser.add_argument("--experiment_name", type=str, help="Experiment Name")
    parser.add_argument("--compute_name", type=str, help="Compute Cluster Name")
    parser.add_argument("--data_name", type=str, help="Data Asset Name")
    parser.add_argument("--model_name", type=str, help="Model Name")
    parser.add_argument("--jobtype", type=str, help="Job Type")
    parser.add_argument("--environment_name", type=str, help="Registered Environment Name")
    parser.add_argument("--version", type=str, help="Version of registered features")
    args = parser.parse_args()
    logger.info(f"Parsed arguments: {args}")
    return args

def create_train_component(parent_dir, jobtype, environment_name):
    """Create the training component"""
    logger.info(f"Creating training component with environment: {environment_name}")
    return command(
        name="train_model_from_patient_features",
        display_name="train-model-from-patient-features",
        code=os.path.join(parent_dir, jobtype),
        command="python train_from_agg_features.py \
                --registered_features ${{inputs.registered_features}} \
                --model_output ${{outputs.model_output}} \
                --model_name ${{inputs.model_name}}",
        environment=environment_name+"@latest",
        inputs={
            "registered_features": Input(type="mltable"),
            "model_name": Input(type="string")
        },
        outputs={
            "model_output": Output(type="uri_folder")
        }
    )
args = parse_args()
@dsl.pipeline(
    description="EEG Model Training Pipeline",
    display_name="EEG-Train-Pipeline" + "-" + args.jobtype
)
def eeg_train_pipeline(registered_features, model_name):
    """Pipeline to train model"""
    logger.info("Initializing EEG training pipeline")
    
    # Training step
    logger.info("Setting up training job")
    train_job = train_model_from_features(
        registered_features=registered_features,
        model_name=model_name
    )
    
    return {
        "trained_model": train_job.outputs.model_output
    }

def main():
    logger.info("Starting training pipeline deployment")
    args = parse_args()
    
    # Set up Azure ML client
    logger.info("Setting up Azure ML client")
    credential = ClientSecretCredential(
        client_id=os.environ["AZURE_CLIENT_ID"],
        client_secret=os.environ["AZURE_CLIENT_SECRET"],
        tenant_id=os.environ["AZURE_TENANT_ID"]
    )
    ml_client = MLClient.from_config(credential=credential)
    
    # Verify compute cluster
    try:
        compute_target = ml_client.compute.get(args.compute_name)
        logger.info(f"Found compute target: {compute_target.name}")
    except Exception as e:
        logger.error(f"Error accessing compute cluster: {str(e)}")
        raise
    
    parent_dir = "amlws-assets/src"
    logger.info(f"Using parent directory: {parent_dir}")
    
    # Create training component
    global train_model_from_features
    train_model_from_features = create_train_component(
        parent_dir, 
        args.jobtype, 
        args.environment_name
    )
    
    # Get the registered MLTable
    logger.info(f"Getting registered features version: {args.version}")
    registered_features = Input(type="mltable", path=f"azureml:eeg_features:{args.version}")
    
    # Create pipeline
    logger.info("Creating pipeline job")
    pipeline_job = eeg_train_pipeline(
        registered_features=registered_features,
        model_name=args.model_name + "-patient"
    )
    
    # Configure pipeline settings
    logger.info("Configuring pipeline settings")
    pipeline_job.settings.default_compute = args.compute_name
    pipeline_job.settings.default_datastore = "workspaceblobstore"
    pipeline_job.settings.continue_on_step_failure = False
    pipeline_job.settings.force_rerun = True
    pipeline_job.settings.default_timeout = 3600
    
    # Submit and monitor pipeline job
    logger.info(f"Submitting pipeline job to experiment: {args.experiment_name}")
    pipeline_job = ml_client.jobs.create_or_update(
        pipeline_job, experiment_name=args.experiment_name
    )
    
    logger.info(f"Pipeline job submitted. Job name: {pipeline_job.name}")
    logger.info("Starting job monitoring...")
    ml_client.jobs.stream(pipeline_job.name)

if __name__ == "__main__":
    main()

================
File: pipelinejobs/eegpreprocess.py
================
import argparse
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import ClientSecretCredential
from azure.ai.ml import MLClient, Input, Output, command
from azure.ai.ml.dsl import pipeline
import os
import json

def parse_args():
    parser = argparse.ArgumentParser("Deploy EEG Analysis Pipeline")
    parser.add_argument("--experiment_name", type=str, help="Experiment Name")
    parser.add_argument("--compute_name", type=str, help="Compute Cluster Name")
    parser.add_argument("--data_name", type=str, help="Data Asset Name")
    parser.add_argument("--model_name", type=str, help="Model Name")
    parser.add_argument("--jobtype", type=str, help="Job Type")
    parser.add_argument("--environment_name", type=str, help="Registered Environment Name")
    parser.add_argument("--sampling_rate", type=int, default=256, help="EEG Sampling Rate")
    parser.add_argument("--cutoff_frequency", type=int, default=60, help="Filter Cutoff Frequency")
    parser.add_argument("--window_seconds", type=int, default=1, help="Window Size in Seconds")
    parser.add_argument("--version", type=str, help="Version of registered features")
    args = parser.parse_args()
    return args

def main():
    args = parse_args()
    print(args)
    
    credential = ClientSecretCredential(
        client_id=os.environ["AZURE_CLIENT_ID"],
        client_secret=os.environ["AZURE_CLIENT_SECRET"],
        tenant_id=os.environ["AZURE_TENANT_ID"]
    )
    ml_client = MLClient.from_config(credential=credential)

    try:
        print(ml_client.compute.get(args.compute_name))
    except:
        print("No compute found")

    parent_dir = "amlws-assets/src"
    
    # Data loading component
    data_loader = command(
        name="data_loader",
        display_name="load-data",
        code=os.path.join(parent_dir, args.jobtype),
        command="python data_loader.py \
                --input_data ${{inputs.input_data}} \
                --output_data ${{outputs.output_data}}",
        environment=args.environment_name+"@latest",
        inputs={
            "input_data": Input(type="uri_file")
        },
        outputs={
            "output_data": Output(type="uri_folder")
        }
    )

    # Upsampling component
    upsampler = command(
        name="upsampler",
        display_name="upsample-data",
        code=os.path.join(parent_dir, args.jobtype),
        command="python upsampler.py \
                --input_data ${{inputs.input_data}} \
                --output_data ${{outputs.output_data}} \
                --upsampling_factor 2",
        environment=args.environment_name+"@latest",
        inputs={
            "input_data": Input(type="uri_folder")
        },
        outputs={
            "output_data": Output(type="uri_folder")
        }
    )

    # Filtering component
    filter_data = command(
        name="filter",
        display_name="filter-data",
        code=os.path.join(parent_dir, args.jobtype),
        command="python filter.py \
                --input_data ${{inputs.input_data}} \
                --output_data ${{outputs.output_data}} \
                --sampling_rate ${{inputs.sampling_rate}} \
                --cutoff_frequency ${{inputs.cutoff_frequency}}",
        environment=args.environment_name+"@latest",
        inputs={
            "input_data": Input(type="uri_folder"),
            "sampling_rate": Input(type="number"),
            "cutoff_frequency": Input(type="number")
        },
        outputs={
            "output_data": Output(type="uri_folder")
        }
    )

    # Downsampling component
    downsampler = command(
        name="downsampler",
        display_name="downsample-data",
        code=os.path.join(parent_dir, args.jobtype),
        command="python downsampler.py \
                --input_data ${{inputs.input_data}} \
                --output_data ${{outputs.output_data}}",
        environment=args.environment_name+"@latest",
        inputs={
            "input_data": Input(type="uri_folder")
        },
        outputs={
            "output_data": Output(type="uri_folder")
        }
    )

    window_slicer = command(
        name="window_slicer",
        display_name="window-slicer",
        code=os.path.join(parent_dir, args.jobtype),
        command="python window_slicer.py \
                --input_data ${{inputs.input_data}} \
                --output_data ${{outputs.output_data}} \
                --window_seconds ${{inputs.window_seconds}} \
                --sampling_rate ${{inputs.sampling_rate}}",
        environment=args.environment_name+"@latest",
        inputs={
            "input_data": Input(type="uri_folder"),
            "sampling_rate": Input(type="number"),
            "window_seconds": Input(type="number")
        },
        outputs={
            "output_data": Output(type="uri_folder")
        }
    )

    # Feature extraction component
    extract_features = command(
        name="extract_features",
        display_name="extract-features",
        code=os.path.join(parent_dir, args.jobtype),
        command="python extract_features.py \
                --processed_data ${{inputs.processed_data}} \
                --features_output ${{outputs.features_output}} \
                --sampling_rate ${{inputs.sampling_rate}}",
        environment=args.environment_name+"@latest",
        inputs={
            "processed_data": Input(type="uri_folder"),
            "sampling_rate": Input(type="number")
        },
        outputs={
            "features_output": Output(type="uri_folder")
        }
    )

    # Pass the JSON string to the register_features command
    register_features = command(
        name="register_features",
        display_name="register-features",
        code=os.path.join(parent_dir, args.jobtype),
        command="python register_features.py \
                --features_input ${{inputs.features_input}} \
                --data_name ${{inputs.data_name}} \
                --registered_features_output ${{outputs.registered_features}} \
                --subscription_id ${{inputs.subscription_id}} \
                --resource_group ${{inputs.resource_group}} \
                --workspace_name ${{inputs.workspace_name}} \
                --client_id ${{inputs.client_id}} \
                --client_secret ${{inputs.client_secret}} \
                --tenant_id ${{inputs.tenant_id}} \
                --version ${{inputs.version}}",
        environment=args.environment_name+"@latest",
        inputs={
            "features_input": Input(type="uri_folder"),
            "data_name": Input(type="string"),
            "subscription_id": Input(type="string"),
            "resource_group": Input(type="string"),
            "workspace_name": Input(type="string"),
            "client_id": Input(type="string"),
            "client_secret": Input(type="string"),
            "tenant_id": Input(type="string"),
            "version": Input(type="string")
        },
        outputs={
            "registered_features": Output(type="uri_file")
        }
    )


    @pipeline(
        description="EEG Analysis Pipeline for Depression Classification",
        display_name="EEG-Analysis-Pipeline"
    )
    def eeg_analysis_pipeline(raw_data, sampling_rate, cutoff_frequency, feature_data_name, window_seconds, version):
        # Load data
        load = data_loader(
            input_data=raw_data
        )

        # Upsample data
        upsampled = upsampler(
            input_data=load.outputs.output_data
        )

        # Apply filtering
        filtered = filter_data(
            input_data=upsampled.outputs.output_data,
            sampling_rate=sampling_rate,
            cutoff_frequency=cutoff_frequency
        )

        # Downsample filtered data
        downsampled = downsampler(
            input_data=filtered.outputs.output_data
        )

        windowed = window_slicer(
            input_data=downsampled.outputs.output_data,
            sampling_rate=sampling_rate,
            window_seconds=window_seconds
        )
        # Extract features
        features = extract_features(
            processed_data=windowed.outputs.output_data,
            sampling_rate=sampling_rate
        )

        registered = register_features(
            features_input=features.outputs.features_output,
            data_name=feature_data_name,
            subscription_id=ml_client.subscription_id,
            resource_group=ml_client.resource_group_name,
            workspace_name=ml_client.workspace_name,
            client_id=os.environ["AZURE_CLIENT_ID"],
            client_secret=os.environ["AZURE_CLIENT_SECRET"],
            tenant_id=os.environ["AZURE_TENANT_ID"],
            version=version
        )

        return {
            "loaded_data": load.outputs.output_data,
            "upsampled_data": upsampled.outputs.output_data,
            "filtered_data": filtered.outputs.output_data,
            "downsampled_data": downsampled.outputs.output_data,
            "windowed_data": windowed.outputs.output_data,
            "features": features.outputs.features_output,
            "registered_features": registered.outputs.registered_features
        }

    # Create pipeline job
    pipeline_job = eeg_analysis_pipeline(
        Input(path=args.data_name + "@latest", type="uri_file"),
        args.sampling_rate,
        args.cutoff_frequency,
        "eeg_features",
        args.window_seconds,
        args.version
    )

    # Set pipeline level compute
    pipeline_job.settings.default_compute = args.compute_name
    # Set pipeline level datastore
    pipeline_job.settings.default_datastore = "workspaceblobstore"
    # Add pipeline settings
    pipeline_job.settings.continue_on_step_failure = False
    pipeline_job.settings.force_rerun = True
    pipeline_job.settings.default_timeout = 3600

    # Submit and monitor pipeline job
    pipeline_job = ml_client.jobs.create_or_update(
        pipeline_job, experiment_name=args.experiment_name
    )
    ml_client.jobs.stream(pipeline_job.name)

if __name__ == "__main__":
    main()

================
File: pipelinejobs/eegwindowrai.py
================
import argparse
from azure.ai.ml.entities import Data, Model
from azure.ai.ml.constants import AssetTypes
from azure.identity import ClientSecretCredential
from azure.ai.ml import MLClient, Input, Output, command, dsl
import os
import json
import uuid
import time
import logging
from typing import Dict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("Deploy EEG Analysis Pipeline")
    parser.add_argument("--experiment_name", type=str, required=True, help="Experiment Name")
    parser.add_argument("--compute_name", type=str, required=True, help="Compute Cluster Name")
    parser.add_argument("--data_name", type=str, required=True, help="Data Asset Name")
    parser.add_argument("--model_name", type=str, required=True, help="Model Name")
    parser.add_argument("--version", type=str, required=True, help="Version of registered features")
    parser.add_argument("--model_version", type=str, required=False, default="latest", help="Version of the registered model")
    parser.add_argument("--environment_name", type=str, required=True, help="Environment name")
    return parser.parse_args()

def setup_rai_components(ml_client_registry):
    """Set up RAI components from registry
    
    Args:
        ml_client_registry: Azure ML registry client
        
    Returns:
        dict: Dictionary containing RAI components
    """
    logger.info("Setting up RAI components...")
    label = "latest"
    
    rai_constructor = ml_client_registry.components.get(
        name="microsoft_azureml_rai_tabular_insight_constructor", 
        label=label
    )
    version = rai_constructor.version
    logger.info(f"Using RAI components version: {version}")
    
    components = {
        'constructor': rai_constructor,
        'error_analysis': ml_client_registry.components.get(
            name="microsoft_azureml_rai_tabular_erroranalysis", 
            version=version
        ),
        'explanation': ml_client_registry.components.get(
            name="microsoft_azureml_rai_tabular_explanation", 
            version=version
        ),
        'gather': ml_client_registry.components.get(
            name="microsoft_azureml_rai_tabular_insight_gather", 
            version=version
        )
    }
    logger.info("RAI components setup complete")
    return components

def create_split_component(environment_name):
    """Create the data splitting component"""
    return command(
        name="split_data",
        display_name="split-data",
        code="amlws-assets/src",
        command="python split_data_window.py \
                --input_mltable ${{inputs.input_mltable}} \
                --train_data ${{outputs.train_data}} \
                --test_data ${{outputs.test_data}} \
                --test_size 0.4",
        environment=environment_name+"@latest",
        inputs={
            "input_mltable": Input(type="mltable")
        },
        outputs={
            "train_data": Output(type="mltable"),
            "test_data": Output(type="mltable")
        }
    )

def create_rai_pipeline(
    compute_name: str,
    model_name: str,
    model_version: str,
    target_column_name: str,
    input_data: Input,
    environment_name: str,
    rai_components: Dict
):
    """Create the RAI pipeline with all components"""
    # Create the split component
    split_data = create_split_component(environment_name)
    
    @dsl.pipeline(
        compute=compute_name,
        description="RAI insights on EEG data",
        experiment_name=f"RAI_insights_{model_name}",
    )
    def rai_decision_pipeline(
        target_column_name, input_data
    ):
        # Split the data
        split_job = split_data(
            input_mltable=input_data
        )
        
        expected_model_id = f"{model_name}:{model_version}"
        azureml_model_id = f"azureml:{expected_model_id}"
        
        create_rai_job = rai_components['constructor'](
            title="RAI dashboard EEG",
            task_type="classification",
            model_info=expected_model_id,
            model_input=Input(type=AssetTypes.MLFLOW_MODEL, path=azureml_model_id),
            train_dataset=split_job.outputs.train_data,
            test_dataset=split_job.outputs.test_data,
            target_column_name=target_column_name,
            classes='[false, true]'
        )
        create_rai_job.set_limits(timeout=300)

        error_job = rai_components['error_analysis'](
            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,
        )
        error_job.set_limits(timeout=300)

        explanation_job = rai_components['explanation'](
            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,
            comment="add explanation", 
        )
        explanation_job.set_limits(timeout=300)

        gather_job = rai_components['gather'](
            constructor=create_rai_job.outputs.rai_insights_dashboard,
            insight_3=error_job.outputs.error_analysis,
            insight_4=explanation_job.outputs.explanation,
        )
        gather_job.set_limits(timeout=300)

        gather_job.outputs.dashboard.mode = "upload"

        return {
            "dashboard": gather_job.outputs.dashboard,
        }
    
    return rai_decision_pipeline(
        target_column_name=target_column_name,
        input_data=input_data
    )

def main():
    try:
        logger.info("Starting RAI pipeline deployment")
        args = parse_args()
        
        # Set up Azure ML client
        logger.info("Setting up Azure ML client")
        credential = ClientSecretCredential(
            client_id=os.environ["AZURE_CLIENT_ID"],
            client_secret=os.environ["AZURE_CLIENT_SECRET"],
            tenant_id=os.environ["AZURE_TENANT_ID"]
        )
        ml_client = MLClient.from_config(credential=credential)
        
        # Verify compute cluster
        compute_target = ml_client.compute.get(args.compute_name)
        logger.info(f"Found compute target: {compute_target.name}")
        
        # Verify model exists
        model = ml_client.models.get(args.model_name + "-window", version=args.model_version)
        logger.info(f"Found model {args.model_name + '-window'} (version {model.version}) with URI: {model.path}")
        
        # Get RAI components from registry
        registry_name = "azureml"
        logger.info(f"Accessing registry: {registry_name}")
        ml_client_registry = MLClient(
            credential=credential,
            subscription_id=ml_client.subscription_id,
            resource_group_name=ml_client.resource_group_name,
            registry_name=registry_name,
        )
        
        # Setup RAI components
        rai_components = setup_rai_components(ml_client_registry)
        
        # Get the registered MLTable
        logger.info(f"Getting registered features version: {args.version}")
        registered_features = Input(
            type="mltable",
            path=f"azureml:{args.data_name}:{args.version}",
            mode="ro_mount"
        )
        
        # Create pipeline
        logger.info("Creating RAI pipeline job")
        pipeline_job = create_rai_pipeline(
            compute_name=args.compute_name,
            model_name=args.model_name + "-window",
            model_version=args.model_version,
            target_column_name="Remission",
            input_data=registered_features,
            environment_name=args.environment_name,
            rai_components=rai_components
        )
        
        # Configure pipeline settings
        logger.info("Configuring pipeline settings")
        pipeline_job.settings.default_compute = args.compute_name
        pipeline_job.settings.default_datastore = "workspaceblobstore"
        pipeline_job.settings.continue_on_step_failure = False
        pipeline_job.settings.force_rerun = True
        pipeline_job.settings.default_timeout = 3600
        
        # Submit and monitor pipeline job
        logger.info(f"Submitting pipeline job to experiment: {args.experiment_name}")
        pipeline_job = ml_client.jobs.create_or_update(
            pipeline_job, experiment_name=args.experiment_name
        )
        
        logger.info(f"Pipeline job submitted. Job name: {pipeline_job.name}")
        logger.info("Starting job monitoring...")
        ml_client.jobs.stream(pipeline_job.name)
        
    except Exception as e:
        logger.error(f"Pipeline deployment failed: {str(e)}")
        raise

if __name__ == "__main__":
    main()

================
File: pipelinejobs/eegwindowtrain.py
================
import argparse
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import ClientSecretCredential
from azure.ai.ml import MLClient, Input, Output, command, dsl
import os
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("Deploy EEG Training Pipeline")
    parser.add_argument("--experiment_name", type=str, help="Experiment Name")
    parser.add_argument("--compute_name", type=str, help="Compute Cluster Name")
    parser.add_argument("--data_name", type=str, help="Data Asset Name")
    parser.add_argument("--model_name", type=str, help="Model Name")
    parser.add_argument("--jobtype", type=str, help="Job Type")
    parser.add_argument("--environment_name", type=str, help="Registered Environment Name")
    parser.add_argument("--version", type=str, help="Version of registered features")
    args = parser.parse_args()
    logger.info(f"Parsed arguments: {args}")
    return args

def create_train_component(parent_dir, jobtype, environment_name):
    """Create the training component"""
    logger.info(f"Creating training component with environment: {environment_name}")
    return command(
        name="train_model_from_window_features",
        display_name="train-model-from-window-features",
        code=os.path.join(parent_dir, jobtype),
        command="python train_from_features.py \
                --registered_features ${{inputs.registered_features}} \
                --model_output ${{outputs.model_output}} \
                --model_name ${{inputs.model_name}}",
        environment=environment_name+"@latest",
        inputs={
            "registered_features": Input(type="mltable"),
            "model_name": Input(type="string")
        },
        outputs={
            "model_output": Output(type="uri_folder")
        }
    )
args = parse_args()
@dsl.pipeline(
    description="EEG Model Training Pipeline",
    display_name="EEG-Train-Pipeline" + "-" + args.jobtype
)
def eeg_train_pipeline(registered_features, model_name):
    """Pipeline to train model"""
    logger.info("Initializing EEG training pipeline")
    
    # Training step
    logger.info("Setting up training job")
    train_job = train_model_from_features(
        registered_features=registered_features,
        model_name=model_name
    )
    
    return {
        "trained_model": train_job.outputs.model_output
    }

def main():
    logger.info("Starting training pipeline deployment")
    args = parse_args()
    
    # Set up Azure ML client
    logger.info("Setting up Azure ML client")
    credential = ClientSecretCredential(
        client_id=os.environ["AZURE_CLIENT_ID"],
        client_secret=os.environ["AZURE_CLIENT_SECRET"],
        tenant_id=os.environ["AZURE_TENANT_ID"]
    )
    ml_client = MLClient.from_config(credential=credential)
    
    # Verify compute cluster
    try:
        compute_target = ml_client.compute.get(args.compute_name)
        logger.info(f"Found compute target: {compute_target.name}")
    except Exception as e:
        logger.error(f"Error accessing compute cluster: {str(e)}")
        raise
    
    parent_dir = "amlws-assets/src"
    logger.info(f"Using parent directory: {parent_dir}")
    
    # Create training component
    global train_model_from_features
    train_model_from_features = create_train_component(
        parent_dir, 
        args.jobtype, 
        args.environment_name
    )
    
    # Get the registered MLTable
    logger.info(f"Getting registered features version: {args.version}")
    registered_features = Input(type="mltable", path=f"azureml:eeg_features:{args.version}")
    
    # Create pipeline
    logger.info("Creating pipeline job")
    pipeline_job = eeg_train_pipeline(
        registered_features=registered_features,
        model_name=args.model_name + "-window"
    )
    
    # Configure pipeline settings
    logger.info("Configuring pipeline settings")
    pipeline_job.settings.default_compute = args.compute_name
    pipeline_job.settings.default_datastore = "workspaceblobstore"
    pipeline_job.settings.continue_on_step_failure = False
    pipeline_job.settings.force_rerun = True
    pipeline_job.settings.default_timeout = 3600
    
    # Submit and monitor pipeline job
    logger.info(f"Submitting pipeline job to experiment: {args.experiment_name}")
    pipeline_job = ml_client.jobs.create_or_update(
        pipeline_job, experiment_name=args.experiment_name
    )
    
    logger.info(f"Pipeline job submitted. Job name: {pipeline_job.name}")
    logger.info("Starting job monitoring...")
    ml_client.jobs.stream(pipeline_job.name)

if __name__ == "__main__":
    main()

================
File: src/multi/data_loader.py
================
# data_loader.py
import argparse
from pathlib import Path
import scipy.io
import mlflow
import logging
import time
import numpy as np
import pandas as pd

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("data_loader")
    parser.add_argument("--input_data", type=str, help="Path to input .mat file")
    parser.add_argument("--output_data", type=str, help="Path to output data")
    args = parser.parse_args()
    return args

def process_eeg_data(eeg_data, group_name):
    """Process EEG data into DataFrame format."""
    processed_frames = []
    channel_names = ['af7', 'af8', 'tp9', 'tp10']
    
    for j in range(eeg_data.shape[0]):
        participant = eeg_data[j, 1][0][:4]
        
        for k in range(eeg_data[j, 0].shape[1]):
            data_dict = {'Participant': participant}
            
            for i, channel in enumerate(channel_names):
                data_dict[channel] = eeg_data[j, 0][0, k][:, i]
            
            processed_frames.append(pd.DataFrame([data_dict]))
    
    return pd.concat(processed_frames, ignore_index=True)

def main():
    mlflow.start_run()
    args = parse_args()
    
    try:
        logger.info(f"Loading MATLAB file: {args.input_data}")
        start_time = time.time()
        mat_data = scipy.io.loadmat(args.input_data)
        load_time = time.time() - start_time
        
        # Process each group
        logger.info("Processing non-remission group...")
        df_non_remission = process_eeg_data(
            mat_data['EEG_windows_Non_remission'],
            'non_remission'
        )
        
        logger.info("Processing remission group...")
        df_remission = process_eeg_data(
            mat_data['EEG_windows_Remission'],
            'remission'
        )
        
        # Log data statistics
        mlflow.log_metric("data_load_time_seconds", load_time)
        mlflow.log_metric("non_remission_participants", len(df_non_remission['Participant'].unique()))
        mlflow.log_metric("remission_participants", len(df_remission['Participant'].unique()))
        mlflow.log_metric("non_remission_windows", len(df_non_remission))
        mlflow.log_metric("remission_windows", len(df_remission))
        
        # Save processed data
        output_path = Path(args.output_data)
        output_path.mkdir(parents=True, exist_ok=True)
        
        df_non_remission.to_parquet(output_path / "non_remission.parquet")
        df_remission.to_parquet(output_path / "remission.parquet")
        
        logger.info("Data loading and processing completed successfully")
        mlflow.log_metric("data_loading_status", 1)
        
    except Exception as e:
        logger.error(f"Error loading data: {str(e)}")
        mlflow.log_metric("data_loading_status", 0)
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()

================
File: src/multi/downsampler.py
================
# downsampler.py
import argparse
from pathlib import Path
import numpy as np
import pandas as pd
from scipy.signal import decimate
import mlflow
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("downsampler")
    parser.add_argument("--input_data", type=str, help="Path to input data directory")
    parser.add_argument("--output_data", type=str, help="Path to output data directory")
    args = parser.parse_args()
    return args

def downsample_data(data):
    """Downsample data by factor of 2 using decimation."""
    try:
        if isinstance(data, (list, np.ndarray)):
            data = np.array(data)
            return decimate(data, 2)  # Includes anti-aliasing filter
        else:
            logger.warning(f"Invalid data type for downsampling: {type(data)}")
            return np.zeros(len(data) // 2)
    except Exception as e:
        logger.warning(f"Error downsampling data: {str(e)}")
        return np.zeros(len(data) // 2)

def process_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Process all channels in a DataFrame."""
    results = []
    channels = ['af7', 'af8', 'tp9', 'tp10']
    
    total_windows = len(df)
    for idx, row in df.iterrows():
        if idx % 100 == 0:
            logger.info(f"Processing window {idx}/{total_windows}")
            
        processed_row = {'Participant': row['Participant']}
        
        for channel in channels:
            channel_data = row[channel]
            downsampled_data = downsample_data(channel_data)
            processed_row[channel] = downsampled_data
            
            # Log downsample ratio for first window
            if idx == 0:
                mlflow.log_metric(f"{channel}_downsample_ratio", 
                                len(downsampled_data) / len(channel_data))
            
        results.append(processed_row)
    
    return pd.DataFrame(results)

def main():
    mlflow.start_run()
    args = parse_args()
    
    try:
        start_time = time.time()
        input_path = Path(args.input_data)
        output_path = Path(args.output_data)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Process each group
        for group in ['non_remission', 'remission']:
            logger.info(f"Processing {group} data...")
            group_start_time = time.time()
            
            # Load data
            input_file = input_path / f"{group}.parquet"
            if not input_file.exists():
                raise FileNotFoundError(f"Input file not found: {input_file}")
            
            df = pd.read_parquet(input_file)
            logger.info(f"Loaded {len(df)} windows for {group}")
            
            # Log pre-downsample statistics
            for channel in ['af7', 'af8', 'tp9', 'tp10']:
                pre_downsample_data = np.vstack(df[channel].values)
                mlflow.log_metric(f"{group}_{channel}_pre_downsample_mean", 
                                np.mean(pre_downsample_data))
                mlflow.log_metric(f"{group}_{channel}_pre_downsample_std", 
                                np.std(pre_downsample_data))
            
            # Process data
            downsampled_df = process_dataframe(df)
            
            # Save processed data
            output_file = output_path / f"{group}.parquet"
            downsampled_df.to_parquet(output_file)
            
            # Log metrics
            group_time = time.time() - group_start_time
            mlflow.log_metric(f"{group}_windows_processed", len(downsampled_df))
            mlflow.log_metric(f"{group}_processing_time", group_time)
            
            # Log post-downsample statistics
            for channel in ['af7', 'af8', 'tp9', 'tp10']:
                post_downsample_data = np.vstack(downsampled_df[channel].values)
                mlflow.log_metric(f"{group}_{channel}_post_downsample_mean", 
                                np.mean(post_downsample_data))
                mlflow.log_metric(f"{group}_{channel}_post_downsample_std", 
                                np.std(post_downsample_data))
        
        # Log execution metrics
        total_time = time.time() - start_time
        mlflow.log_metric("total_processing_time", total_time)
        mlflow.log_metric("downsampling_status", 1)
        
        logger.info(f"Downsampling completed in {total_time:.2f} seconds")
        
    except Exception as e:
        logger.error(f"Error in downsampling: {str(e)}")
        mlflow.log_metric("downsampling_status", 0)
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()

================
File: src/multi/extract_features.py
================
import argparse
from pathlib import Path
import numpy as np
import pandas as pd
from scipy.signal import welch
from scipy.stats import skew, kurtosis, entropy
import nolds
import mlflow
import logging
import time
from typing import Dict, Any, List, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("extract_features")
    parser.add_argument("--processed_data", type=str, help="Path to processed data")
    parser.add_argument("--features_output", type=str, help="Path to features output")
    parser.add_argument("--sampling_rate", type=int, default=256)
    args = parser.parse_args()
    return args

def validate_data(data: np.ndarray) -> Tuple[bool, str]:
    """Validate data before feature extraction."""
    try:
        if not isinstance(data, np.ndarray):
            return False, "Data is not a numpy array"
        if len(data) == 0:
            return False, "Empty data array"
        if np.any(np.isnan(data)):
            return False, "Data contains NaN values"
        if np.any(np.isinf(data)):
            return False, "Data contains infinite values"
        return True, "Data validation passed"
    except Exception as e:
        return False, f"Validation error: {str(e)}"

def band_power(data: np.ndarray, sf: int, band: tuple, window_length: int) -> float:
    """Calculate power in a specific frequency band with validation."""
    try:
        valid, message = validate_data(data)
        if not valid:
            logger.warning(f"Band power calculation: {message}")
            return np.nan
            
        f, Pxx = welch(data, sf, nperseg=min(window_length, len(data)))
        ind_min = np.argmax(f > band[0]) - 1
        ind_max = np.argmax(f > band[1]) - 1
        power = np.trapz(Pxx[ind_min: ind_max], f[ind_min: ind_max])
        
        # Validate power calculation
        if np.isnan(power) or np.isinf(power):
            logger.warning("Invalid power value calculated")
            return np.nan
            
        return power
    except Exception as e:
        logger.warning(f"Error calculating band power: {str(e)}")
        return np.nan

def hfd(data, Kmax):
    # Initialize an empty list to store log-log values
    x = []
    # Get the length of the input data
    N = len(data)
    # Loop over each scale from 1 to Kmax
    for k in range(1, Kmax + 1):
        # Initialize an empty list to store Lmk values for the current scale
        Lk = []
        # Loop over each segment within the current scale
        for m in range(k):
            # Calculate indices for the current segment
            indices = np.arange(m, N, k)
            # Skip if the segment has fewer than 2 points
            if len(indices) < 2:
                continue
            # Calculate the sum of absolute differences for the segment
            Lmk = np.sum(np.abs(np.diff(data[indices])))
            # Normalize Lmk by the segment length and scale
            Lmk *= (N - 1) / ((len(indices) - 1) * k)
            # Append the normalized Lmk to the list for the current scale
            Lk.append(Lmk)
        # If there are valid Lmk values, calculate log-log values and append to x
        if len(Lk) > 0:
            x.append([np.log(1.0 / k), np.log(np.mean(Lk))])

    # Convert x to a numpy array for linear fitting
    x = np.array(x)
    # Perform a linear fit to the log-log values to determine the slope
    a, b = np.polyfit(x[:, 0], x[:, 1], 1)
    # Return the slope, which is the estimate of the fractal dimension
    return a

def compute_entropy_features(data: np.ndarray) -> Dict[str, float]:
    """Compute entropy-based features."""
    try:
        valid, message = validate_data(data)
        if not valid:
            logger.warning(f"Entropy calculation: {message}")
            return {k: np.nan for k in ['sample_entropy', 'spectral_entropy']}
            
        # Sample entropy
        sample_entropy = nolds.sampen(data)
        
        # Spectral entropy
        f, Pxx = welch(data, nperseg=min(len(data), 256))
        psd_norm = Pxx / np.sum(Pxx)
        spectral_entropy = entropy(psd_norm)
        
        return {
            'sample_entropy': sample_entropy,
            'spectral_entropy': spectral_entropy
        }
    except Exception as e:
        logger.warning(f"Error computing entropy features: {str(e)}")
        return {k: np.nan for k in ['sample_entropy', 'spectral_entropy']}

def compute_complexity_measures(data: np.ndarray) -> Dict[str, float]:
    """Compute various complexity measures with enhanced validation."""
    try:
        valid, message = validate_data(data)
        if not valid:
            logger.warning(f"Complexity calculation: {message}")
            return {k: np.nan for k in ['hfd', 'corr_dim', 'hurst', 'lyap_r', 'dfa']}
        
        # Ensure data is float64 type and handle NaN/inf values
        data = np.array(data, dtype=np.float64)
        if len(data) < 50:  # Minimum length for reliability
            logger.warning("Data length too short for complexity measures")
            return {k: np.nan for k in ['hfd', 'corr_dim', 'hurst', 'lyap_r', 'dfa']}
        
        features = {}
        
        # Higuchi Fractal Dimension
        try:
            features['hfd'] = hfd(data, Kmax=10)
        except Exception as e:
            logger.warning(f"HFD calculation failed: {str(e)}")
            features['hfd'] = np.nan
        
        # Correlation Dimension
        try:
            features['corr_dim'] = nolds.corr_dim(data, emb_dim=10)
        except Exception as e:
            logger.warning(f"Correlation dimension calculation failed: {str(e)}")
            features['corr_dim'] = np.nan
        
        # Hurst Exponent
        try:
            features['hurst'] = nolds.hurst_rs(data)
        except Exception as e:
            logger.warning(f"Hurst exponent calculation failed: {str(e)}")
            features['hurst'] = np.nan
        
        # Largest Lyapunov Exponent
        try:
            # Normalize and prepare data
            data_norm = (data - np.mean(data)) / (np.std(data) + 1e-10)
            data_norm = np.ascontiguousarray(data_norm, dtype=np.float64)
            
            # Calculate embedding parameters
            emb_dim = 10
            lag = max(1, int(len(data_norm) // 20))  # Use integer division
            
            # Ensure minimum data length
            min_length = (emb_dim - 1) * lag + 1
            if len(data_norm) < min_length:
                logger.warning(f"Data length {len(data_norm)} insufficient for lyap_r calculation")
                features['lyap_r'] = np.nan
            else:
                logger.info(f"Computing lyap_r with emb_dim={emb_dim}, lag={lag}, data_length={len(data_norm)}")
                features['lyap_r'] = nolds.lyap_r(data_norm, emb_dim=emb_dim, lag=lag, min_tsep=lag)
                
                if not np.isfinite(features['lyap_r']):
                    logger.warning(f"Computed lyap_r is not finite: {features['lyap_r']}")
                    features['lyap_r'] = np.nan
                    
        except Exception as e:
            logger.warning(f"Lyapunov exponent calculation failed: {str(e)}")
            features['lyap_r'] = np.nan
        
        # Detrended Fluctuation Analysis
        try:
            features['dfa'] = nolds.dfa(data)
        except Exception as e:
            logger.warning(f"DFA calculation failed: {str(e)}")
            features['dfa'] = np.nan
        
        # Final validation
        for key, value in features.items():
            if not np.isfinite(value):
                features[key] = np.nan
                logger.warning(f"Replacing {key} with nan due to non-finite value")
        
        return features
            
    except Exception as e:
        logger.warning(f"Error computing complexity measures: {str(e)}")
        return {k: np.nan for k in ['hfd', 'corr_dim', 'hurst', 'lyap_r', 'dfa']}


def compute_statistical_features(data: np.ndarray) -> Dict[str, float]:
    """Compute statistical features with validation."""
    try:
        valid, message = validate_data(data)
        if not valid:
            logger.warning(f"Statistical calculation: {message}")
            return {k: np.nan for k in ['mean', 'std', 'skewness', 'kurtosis', 'rms',
                                      'zero_crossings', 'peak_to_peak', 'variance',
                                      'mean_abs_deviation']}
        
        return {
            'mean': np.mean(data),
            'std': np.std(data),
            'variance': np.var(data),
            'skewness': skew(data),
            'kurtosis': kurtosis(data),
            'rms': np.sqrt(np.mean(np.square(data))),
            'zero_crossings': np.sum(np.diff(np.signbit(data))),
            'peak_to_peak': np.ptp(data),
            'mean_abs_deviation': np.mean(np.abs(data - np.mean(data)))
        }
    except Exception as e:
        logger.warning(f"Error computing statistical features: {str(e)}")
        return {k: np.nan for k in ['mean', 'std', 'skewness', 'kurtosis', 'rms',
                                   'zero_crossings', 'peak_to_peak', 'variance',
                                   'mean_abs_deviation']}

def compute_features(channel_data: np.ndarray, sf: int) -> Dict[str, Any]:
    """Compute all features for a channel with comprehensive logging."""
    # Ensure data is properly formatted
    try:
        if not isinstance(channel_data, np.ndarray):
            logger.warning("Data is not a NumPy array. Converting...")
            channel_data = np.array(channel_data, dtype=np.float64)
        elif channel_data.dtype != np.float64:
            logger.warning(f"Data type is {channel_data.dtype}. Converting to float64...")
            channel_data = channel_data.astype(np.float64)
        if np.any(np.isnan(channel_data)) or np.any(np.isinf(channel_data)):
            logger.warning("Channel data contains NaN or Inf values")
            channel_data = np.nan_to_num(channel_data, nan=0.0, posinf=0.0, neginf=0.0)
    except Exception as e:
        logger.error(f"Error converting channel data: {str(e)}")
        return {}, {}

    features = {}
    feature_computation_times = {}
    
    # Frequency bands
    bands = {
        'delta': (0.5, 4),
        'theta': (4, 8),
        'alpha': (8, 12),
        'beta': (12, 30),
        'gamma': (30, 60)
    }
    
    # Calculate band powers
    start_time = time.time()
    for band_name, band_range in bands.items():
        features[f'bp_{band_name}'] = band_power(channel_data, sf, band_range, len(channel_data))
    feature_computation_times['band_powers'] = time.time() - start_time
    
    # Add complexity measures
    start_time = time.time()
    features.update(compute_complexity_measures(channel_data))
    feature_computation_times['complexity'] = time.time() - start_time
    
    # Add statistical features
    start_time = time.time()
    features.update(compute_statistical_features(channel_data))
    feature_computation_times['statistical'] = time.time() - start_time
    
    # Add entropy features
    start_time = time.time()
    features.update(compute_entropy_features(channel_data))
    feature_computation_times['entropy'] = time.time() - start_time
    
    
    return features, feature_computation_times

def extract_features_from_df(df: pd.DataFrame, sf: int) -> pd.DataFrame:
    """Extract features from all channels with comprehensive logging."""
    all_features = []
    channels = ['af7', 'af8', 'tp9', 'tp10']
    
    feature_timings = {channel: {} for channel in channels}
    missing_features = {channel: 0 for channel in channels}
    
    total_windows = len(df)
    for index, row in df.iterrows():
        if index % 10 == 0:
            logger.info(f"Processing window {index+1}/{total_windows}")
        
        features = {'Participant': row['Participant']}
        
        for channel in channels:
            channel_data = np.array(row[channel])
            channel_features, computation_times = compute_features(channel_data, sf)
            
            # Track timing statistics
            for feature_type, time_taken in computation_times.items():
                if feature_type not in feature_timings[channel]:
                    feature_timings[channel][feature_type] = []
                feature_timings[channel][feature_type].append(time_taken)
            
            # Track missing features
            missing_count = sum(1 for v in channel_features.values() if pd.isna(v))
            missing_features[channel] += missing_count
            
            # Add channel prefix to features
            for feature_name, value in channel_features.items():
                features[f'{channel}_{feature_name}'] = value
        
        all_features.append(features)
    
    # Log detailed metrics
    for channel in channels:
        for feature_type, timings in feature_timings[channel].items():
            mlflow.log_metric(f"{channel}_{feature_type}_mean_time", np.mean(timings))
            mlflow.log_metric(f"{channel}_{feature_type}_max_time", np.max(timings))
        mlflow.log_metric(f"{channel}_missing_features", missing_features[channel])
    
    return pd.DataFrame(all_features)

def main():
    mlflow.start_run()
    args = parse_args()
    
    try:
        start_time = time.time()
        
        # Load processed data
        input_path = Path(args.processed_data)
        df_non_remission = pd.read_parquet(input_path / "non_remission.parquet")
        df_remission = pd.read_parquet(input_path / "remission.parquet")
        
        # Extract features
        logger.info("Extracting features for non-remission group...")
        df_non_remission_features = extract_features_from_df(df_non_remission, args.sampling_rate)
        df_non_remission_features['Remission'] = 0
        
        logger.info("Extracting features for remission group...")
        df_remission_features = extract_features_from_df(df_remission, args.sampling_rate)
        df_remission_features['Remission'] = 1
        
        # Combine datasets
        df_combined = pd.concat([df_non_remission_features, df_remission_features], 
                              ignore_index=True)
        
        # Calculate and log feature statistics
        feature_cols = df_combined.drop(['Participant', 'Remission'], axis=1).columns
        for col in feature_cols:
            mlflow.log_metric(f"feature_{col}_mean", df_combined[col].mean())
            mlflow.log_metric(f"feature_{col}_std", df_combined[col].std())
            mlflow.log_metric(f"feature_{col}_missing_pct", 
                            (df_combined[col].isna().sum() / len(df_combined)) * 100)
        
        # Save features
        output_path = Path(args.features_output)
        output_path.mkdir(parents=True, exist_ok=True)
        df_combined.to_parquet(output_path / "features.parquet")
        
        # Create MLTable file
        mltable_content = """
        paths:
          - file: ./features.parquet
        transformations:
          - read_parquet:
              include_path_column: false
        """

        # Write MLTable file
        with open(output_path / "MLTable", "w") as f:
            f.write(mltable_content)
        
        # Log execution metrics
        total_time = time.time() - start_time
        mlflow.log_metric("total_features", len(feature_cols))
        mlflow.log_metric("total_samples", len(df_combined))
        mlflow.log_metric("feature_extraction_time", total_time)
        mlflow.log_metric("features_per_second", len(df_combined) / total_time)
        mlflow.log_metric("feature_extraction_status", 1)
        
        logger.info(f"Features saved to: {args.features_output}")
        logger.info(f"Extracted {len(feature_cols)} features for {len(df_combined)} samples")
        
    except Exception as e:
        logger.error(f"Error in feature extraction: {str(e)}")
        mlflow.log_metric("feature_extraction_status", 0)
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()

================
File: src/multi/filter.py
================
import argparse
from pathlib import Path
import numpy as np
import pandas as pd
from scipy.signal import butter, filtfilt
import mlflow
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("filter")
    parser.add_argument("--input_data", type=str, help="Path to input data directory")
    parser.add_argument("--output_data", type=str, help="Path to output data directory")
    parser.add_argument("--sampling_rate", type=int, default=256)
    parser.add_argument("--cutoff_frequency", type=int, default=60)
    parser.add_argument("--filter_order", type=int, default=4)
    args = parser.parse_args()
    return args

def design_filter(sampling_rate, cutoff_frequency, filter_order):
    """Design Butterworth low-pass filter."""
    nyquist = 0.5 * sampling_rate
    normalized_cutoff = cutoff_frequency / nyquist
    return butter(filter_order, normalized_cutoff, btype='low')

def apply_filter(data, b, a):
    """Apply filter to a single array of data."""
    try:
        if isinstance(data, (list, np.ndarray)):
            data = np.array(data)
            # Apply filter with zero-phase forward and reverse digital filtering
            filtered_data = filtfilt(b, a, data)
            return filtered_data
        else:
            logger.warning(f"Invalid data type for filtering: {type(data)}")
            return np.zeros_like(data)
    except Exception as e:
        logger.warning(f"Error filtering data: {str(e)}")
        return np.zeros_like(data)

def process_dataframe(df: pd.DataFrame, b: np.ndarray, a: np.ndarray) -> pd.DataFrame:
    """Process all channels in a DataFrame."""
    results = []
    channels = ['af7', 'af8', 'tp9', 'tp10']
    
    total_windows = len(df)
    for idx, row in df.iterrows():
        if idx % 100 == 0:
            logger.info(f"Processing window {idx}/{total_windows}")
            
        processed_row = {'Participant': row['Participant']}
        
        for channel in channels:
            channel_data = row[channel]
            filtered_data = apply_filter(channel_data, b, a)
            processed_row[channel] = filtered_data
            
            # Calculate signal power reduction for first window
            if idx == 0:
                original_power = np.mean(np.square(channel_data))
                filtered_power = np.mean(np.square(filtered_data))
                mlflow.log_metric(f"{channel}_power_reduction_db", 
                                10 * np.log10(filtered_power / original_power))
            
        results.append(processed_row)
    
    return pd.DataFrame(results)

def main():
    mlflow.start_run()
    args = parse_args()
    
    try:
        start_time = time.time()
        input_path = Path(args.input_data)
        output_path = Path(args.output_data)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Design filter
        b, a = design_filter(args.sampling_rate * 2,  # Account for upsampled rate
                           args.cutoff_frequency, 
                           args.filter_order)
        
        mlflow.log_params({
            "sampling_rate": args.sampling_rate * 2,
            "cutoff_frequency": args.cutoff_frequency,
            "filter_order": args.filter_order
        })
        
        # Process each group
        for group in ['non_remission', 'remission']:
            logger.info(f"Processing {group} data...")
            group_start_time = time.time()
            
            # Load data
            input_file = input_path / f"{group}.parquet"
            if not input_file.exists():
                raise FileNotFoundError(f"Input file not found: {input_file}")
            
            df = pd.read_parquet(input_file)
            logger.info(f"Loaded {len(df)} windows for {group}")
            
            # Log pre-filtering statistics
            for channel in ['af7', 'af8', 'tp9', 'tp10']:
                pre_filter_data = np.vstack(df[channel].values)
                mlflow.log_metric(f"{group}_{channel}_pre_filter_mean", 
                                np.mean(pre_filter_data))
                mlflow.log_metric(f"{group}_{channel}_pre_filter_std", 
                                np.std(pre_filter_data))
            
            # Apply filtering
            filtered_df = process_dataframe(df, b, a)
            
            # Save processed data
            output_file = output_path / f"{group}.parquet"
            filtered_df.to_parquet(output_file)
            
            # Log metrics
            group_time = time.time() - group_start_time
            mlflow.log_metric(f"{group}_windows_processed", len(filtered_df))
            mlflow.log_metric(f"{group}_processing_time", group_time)
            
            # Log post-filtering statistics
            for channel in ['af7', 'af8', 'tp9', 'tp10']:
                post_filter_data = np.vstack(filtered_df[channel].values)
                mlflow.log_metric(f"{group}_{channel}_post_filter_mean", 
                                np.mean(post_filter_data))
                mlflow.log_metric(f"{group}_{channel}_post_filter_std", 
                                np.std(post_filter_data))
        
        # Log execution metrics
        total_time = time.time() - start_time
        mlflow.log_metric("total_processing_time", total_time)
        mlflow.log_metric("filtering_status", 1)
        
        logger.info(f"Filtering completed in {total_time:.2f} seconds")
        
    except Exception as e:
        logger.error(f"Error in filtering: {str(e)}")
        mlflow.log_metric("filtering_status", 0)
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()

================
File: src/multi/register_features.py
================
# register_features.py
import argparse
from pathlib import Path
import pandas as pd
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import ClientSecretCredential
import os
import mlflow
import logging
import time
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("register_features")
    parser.add_argument("--features_input", type=str, help="Path to features parquet file")
    parser.add_argument("--data_name", type=str, help="Name for registered data asset")
    parser.add_argument("--description", type=str, default="EEG features for depression classification")
    parser.add_argument("--registered_features_output", type=str, help="Directory path to save registered features info")
    parser.add_argument("--subscription_id", type=str, help="Azure subscription ID")
    parser.add_argument("--resource_group", type=str, help="Azure resource group name")
    parser.add_argument("--workspace_name", type=str, help="Azure ML workspace name")
    parser.add_argument("--client_id", type=str, help="Azure client ID")
    parser.add_argument("--client_secret", type=str, help="Azure client secret")
    parser.add_argument("--tenant_id", type=str, help="Azure tenant ID")
    parser.add_argument("--version", type=str, help="Version of registered features")
    args = parser.parse_args()
    return args

def main():
    mlflow.start_run()
    args = parse_args()
    
    try:
        start_time = time.time()
        
        # Initialize MLClient
        logger.info("Initializing MLClient...")
        credential = ClientSecretCredential(
            client_id=args.client_id,
            client_secret=args.client_secret,
            tenant_id=args.tenant_id
        )
        ml_client = MLClient(
            credential=credential,
            subscription_id=args.subscription_id,
            resource_group_name=args.resource_group,
            workspace_name=args.workspace_name
        )
        
        logger.info(f"Registering features from: {args.features_input}")
        
        # Create MLTable definition
        mltable_data = Data(
            path=args.features_input,
            type=AssetTypes.MLTABLE,
            description=args.description,
            name=args.data_name,
            version=args.version
        )
        
        # Register the data
        registered_data = ml_client.data.create_or_update(mltable_data)
        
        # Save registration information
        registration_info = {
            "name": registered_data.name,
            "version": registered_data.version,
            "id": registered_data.id,
            "path": str(Path(args.features_input))
        }
        
        output_path = Path(args.registered_features_output)
        output_path.mkdir(parents=True, exist_ok=True)
        
        with open(output_path / "registration_info.json", 'w') as f:
            json.dump(registration_info, f)
        
        # Log registration metrics
        mlflow.log_metric("registration_time", time.time() - start_time)
        mlflow.log_metric("registration_status", 1)
        
        logger.info(f"Features registered as MLTable: {registered_data.name}, version: {registered_data.version}")
        
    except Exception as e:
        logger.error(f"Error registering features: {str(e)}")
        mlflow.log_metric("registration_status", 0)
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()

================
File: src/multi/train_from_agg_features.py
================
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from mlflow.models.signature import infer_signature
import logging
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import SGDClassifier

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def aggregate_windows_to_patient(df):
    """
    Aggregate window-level features to patient-level features.
    """
    feature_cols = df.columns.difference(['Participant', 'Remission'])
    
    # Define aggregation functions
    agg_funcs = {col: ['mean', 'std', 'min', 'max', 'median'] 
                 for col in feature_cols}
    agg_funcs['Remission'] = 'first'  # All windows for a patient have same label
    
    # Aggregate
    patient_df = df.groupby('Participant').agg(agg_funcs)
    
    # Add percentiles
    percentiles = [25, 75]
    for col in feature_cols:
        for p in percentiles:
            patient_df[(col, f'percentile_{p}')] = df.groupby('Participant')[col].quantile(p/100)
    
    # Flatten column names
    patient_df.columns = [f"{col}_{agg}" if agg != 'first' else col 
                         for col, agg in patient_df.columns]
    
    # Add number of windows as a feature
    patient_df['n_windows'] = df.groupby('Participant').size()
    
    # Ensure all numeric columns are float64
    numeric_cols = patient_df.select_dtypes(include=['number']).columns
    patient_df[numeric_cols] = patient_df[numeric_cols].astype('float64')
    
    return patient_df.reset_index()

def calculate_detailed_metrics(y_true, y_pred):
    """Calculate detailed metrics with counts"""
    # Basic counts
    TP = sum((y_true == 1) & (y_pred == 1))
    TN = sum((y_true == 0) & (y_pred == 0))
    FP = sum((y_true == 0) & (y_pred == 1))
    FN = sum((y_true == 1) & (y_pred == 0))
    
    # Calculate metrics
    accuracy = (TP + TN) / len(y_true)
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'TP': TP,
        'TN': TN,
        'FP': FP,
        'FN': FN,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'n_samples': len(y_true)
    }

def get_classifiers():
    """Return dictionary of sklearn classifiers to try"""
    # Calculate class weights based on rough class distribution
    class_weights = {0: 1, 1: 2}  # Give more weight to minority class
    
    return {
        'random_forest': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', RandomForestClassifier(
                n_estimators=200,
                min_samples_leaf=2,
                class_weight=class_weights,
                random_state=42
            ))
        ]),
        'gradient_boosting': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', GradientBoostingClassifier(
                n_estimators=200,
                min_samples_leaf=1,
                max_depth=5,
                random_state=42
            ))
        ]),
        'logistic_regression': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', LogisticRegression(
                max_iter=1000,
                class_weight=class_weights,
                random_state=42
            ))
        ]),
        'extra_trees': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', ExtraTreesClassifier(
                n_estimators=200,
                min_samples_leaf=2,
                class_weight=class_weights,
                random_state=42
            ))
        ]),
        'ada_boost': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', AdaBoostClassifier(
                n_estimators=100,
                learning_rate=0.1,
                random_state=42
            ))
        ]),
        'svm_rbf': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', SVC(
                kernel='rbf',
                class_weight=class_weights,
                probability=True,
                random_state=42
            ))
        ]),
        'svm_linear': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', SVC(
                kernel='linear',
                class_weight=class_weights,
                probability=True,
                random_state=42
            ))
        ]),
        'knn': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', KNeighborsClassifier(
                n_neighbors=3,  # Small number due to few samples
                weights='distance'
            ))
        ]),
        'decision_tree': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', DecisionTreeClassifier(
                min_samples_leaf=2,
                class_weight=class_weights,
                random_state=42
            ))
        ]),
        'logistic_regression_l1': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', LogisticRegression(
                penalty='l1',
                solver='liblinear',
                class_weight=class_weights,
                max_iter=1000,
                random_state=42
            ))
        ]),
        'sgd': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', SGDClassifier(
                loss='modified_huber',  # Enables probability estimates
                penalty='elasticnet',
                class_weight=class_weights,
                max_iter=1000,
                random_state=42
            ))
        ])
    }

def parse_args():
    parser = argparse.ArgumentParser("train_from_features")
    parser.add_argument("--registered_features", type=str, help="Path to features dataset")
    parser.add_argument("--model_output", type=str, help="Path of output model")
    parser.add_argument("--model_name", type=str, help="Model name")
    return parser.parse_args()

def main(args):
    logger.info(f"Loading training data from: {args.registered_features}")
    df = pd.read_parquet(Path(args.registered_features) / "features.parquet")
    
    # Aggregate to patient level first
    logger.info("Aggregating window-level features to patient-level...")
    patient_df = aggregate_windows_to_patient(df)
    logger.info(f"Created {patient_df.shape[1]} patient-level features")
    
    # Prepare for training
    X = patient_df.drop(['Participant', 'Remission'], axis=1)
    y = patient_df['Remission']
    groups = patient_df['Participant']
    
    # Calculate and log class distribution
    n_remission_patients = sum(y == 1)
    n_non_remission_patients = sum(y == 0)
    
    # Log detailed dataset statistics
    logger.info("\nDataset Statistics:")
    logger.info(f"Total number of patients: {len(groups)}")
    logger.info(f"- Remission patients: {n_remission_patients}")
    logger.info(f"- Non-remission patients: {n_non_remission_patients}")
    
    # Get classifiers to try
    classifiers = get_classifiers()
    
    # Initialize LOGO cross-validation
    logo = LeaveOneGroupOut()
    n_splits = logo.get_n_splits(X, y, groups)
    logger.info(f"\nPerforming Leave-One-Group-Out cross-validation with {n_splits} splits")
    
    # Dictionary to store results for each classifier
    all_results = {name: [] for name in classifiers.keys()}
    
    # Perform LOGO cross-validation
    for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):
        test_participant = groups.iloc[test_idx]
        true_label = y.iloc[test_idx].iloc[0]
        
        logger.info(f"\nFold {fold_idx + 1}/{n_splits}")
        logger.info(f"Testing on participant: {test_participant.iloc[0]} (true label: {true_label})")
        
        # Split data
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        
        # Try each classifier
        for name, clf in classifiers.items():
            logger.info(f"\nTraining {name}")
            
            # Train the model
            clf.fit(X_train, y_train)
            
            # Make predictions
            pred_prob = clf.predict_proba(X_test)[:, 1][0]
            pred_label = clf.predict(X_test)[0]
            
            # Store results
            all_results[name].append({
                'participant': test_participant.iloc[0],
                'true_label': true_label,
                'predicted_label': pred_label,
                'confidence': pred_prob,
                'correct_prediction': true_label == pred_label
            })
            
            # Log fold metrics
            mlflow.log_metric(f"{name}_fold_{fold_idx}_accuracy", 
                            int(all_results[name][-1]['correct_prediction']))
            mlflow.log_metric(f"{name}_fold_{fold_idx}_confidence", 
                            all_results[name][-1]['confidence'])
    
    # Calculate and log overall metrics for each classifier
    best_classifier = None
    best_f1 = -1
    
    for name in classifiers.keys():
        # Combine results
        results_df = pd.DataFrame(all_results[name])
        
        # Calculate detailed metrics
        metrics = calculate_detailed_metrics(
            results_df['true_label'],
            results_df['predicted_label']
        )
        
        # Log detailed metrics for each classifier
        logger.info(f"\nDetailed Metrics for {name}:")
        logger.info(f"True Positives (Correct Remission): {metrics['TP']}")
        logger.info(f"True Negatives (Correct Non-Remission): {metrics['TN']}")
        logger.info(f"False Positives (Incorrect Remission): {metrics['FP']}")
        logger.info(f"False Negatives (Missed Remission): {metrics['FN']}")
        logger.info(f"Accuracy: {metrics['accuracy']:.3f}")
        logger.info(f"Precision: {metrics['precision']:.3f}")
        logger.info(f"Recall: {metrics['recall']:.3f}")
        logger.info(f"F1 Score: {metrics['f1']:.3f}")
        
        # Log metrics to MLflow
        for metric_name, value in metrics.items():
            if isinstance(value, (int, float)):  # Only log numeric metrics
                mlflow.log_metric(f"{name}_{metric_name}", value)
        
        # Save predictions
        results_df.to_csv(Path(args.model_output) / f'{name}_predictions.csv', index=False)
        
        # Track best classifier based on F1 score
        if metrics['f1'] > best_f1:
            best_f1 = metrics['f1']
            best_classifier = name
    
    logger.info(f"\nBest performing classifier: {best_classifier} (F1: {best_f1:.3f})")
    
    # Train final model on all data using best classifier
    logger.info("\nTraining final model on all data...")
    final_model = classifiers[best_classifier]
    final_model.fit(X, y)
    
    # Save feature importances if available
    if hasattr(final_model.named_steps['clf'], 'feature_importances_'):
        feature_importance_df = pd.DataFrame({
            'feature': X.columns,
            'importance': final_model.named_steps['clf'].feature_importances_
        }).sort_values('importance', ascending=False)
        
        # Save feature importances
        feature_importance_df.to_csv(Path(args.model_output) / 'feature_importances.csv', index=False)
        
        # Log top features
        logger.info("\nTop 10 most important features:")
        for _, row in feature_importance_df.head(10).iterrows():
            logger.info(f"{row['feature']}: {row['importance']:.4f}")
            mlflow.log_metric(f"feature_importance_{row['feature']}", row['importance'])
    
    # Save model to a separate model directory
    model_save_path = Path(args.model_output) / 'model'
    
    # Save model using sklearn format
    signature = infer_signature(X, final_model.predict_proba(X)[:, 1])
    mlflow.sklearn.save_model(
        sk_model=final_model,
        path=model_save_path,
        signature=signature
    )
    
    # Log the model in MLflow
    mlflow.sklearn.log_model(
        sk_model=final_model,
        artifact_path="model",
        registered_model_name=args.model_name,
        signature=signature
    )
    
    # Log model parameters
    for param_name, param_value in final_model.named_steps['clf'].get_params().items():
        mlflow.log_param(f"best_model_{param_name}", param_value)
    
    # Log final metrics
    logger.info("\nMisclassified Patients:")
    results_df = pd.DataFrame(all_results[best_classifier])
    misclassified = results_df[results_df['true_label'] != results_df['predicted_label']]
    for _, row in misclassified.iterrows():
        logger.info(
            f"Participant {row['participant']}: "
            f"True={row['true_label']}, Pred={row['predicted_label']}, "
            f"Confidence={row['confidence']:.3f}"
        )
    
    logger.info("Training completed successfully")

if __name__ == "__main__":
    mlflow.start_run()
    args = parse_args()
    main(args)
    mlflow.end_run()

================
File: src/multi/train_from_features.py
================
import argparse
from pathlib import Path
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from mlflow.models.signature import infer_signature
import logging
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def calculate_patient_prediction(window_predictions, threshold=0.5):
    """
    Convert window-level predictions to a patient-level prediction.
    Returns both the majority vote prediction and the confidence (proportion of windows predicted as positive)
    """
    proportion_positive = np.mean(window_predictions)
    prediction = 1 if proportion_positive >= threshold else 0
    return prediction, proportion_positive

def calculate_detailed_metrics(y_true, y_pred):
    """Calculate detailed metrics with counts"""
    # Basic counts
    TP = sum((y_true == 1) & (y_pred == 1))
    TN = sum((y_true == 0) & (y_pred == 0))
    FP = sum((y_true == 0) & (y_pred == 1))
    FN = sum((y_true == 1) & (y_pred == 0))
    
    # Calculate metrics
    accuracy = (TP + TN) / len(y_true)
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'TP': TP,
        'TN': TN,
        'FP': FP,
        'FN': FN,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'n_samples': len(y_true)
    }

def get_classifiers():
    """Return dictionary of sklearn classifiers to try"""
    # Calculate class weights based on rough class distribution
    class_weights = {0: 1, 1: 2}  # Give more weight to minority class
    
    return {
        'random_forest': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', RandomForestClassifier(
                n_estimators=200,
                min_samples_leaf=20,
                class_weight=class_weights,
                random_state=42
            ))
        ]),
        'gradient_boosting': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', GradientBoostingClassifier(
                n_estimators=200,
                min_samples_leaf=10,
                max_depth=5,
                random_state=42
            ))
        ]),
        'logistic_regression': Pipeline([
            ('scaler', StandardScaler()),
            ('clf', LogisticRegression(
                max_iter=1000,
                class_weight=class_weights,
                random_state=42
            ))
        ])
    }

def parse_args():
    parser = argparse.ArgumentParser("train_from_features")
    parser.add_argument("--registered_features", type=str, help="Path to features dataset")
    parser.add_argument("--model_output", type=str, help="Path of output model")
    parser.add_argument("--model_name", type=str, help="Model name")
    return parser.parse_args()

def main(args):
    logger.info(f"Loading training data from: {args.registered_features}")
    df = pd.read_parquet(Path(args.registered_features) / "features.parquet")
    
    # Prepare for training
    X = df.drop(['Participant', 'Remission'], axis=1)
    y = df['Remission']
    groups = df['Participant']
    
    # Calculate and log class distribution
    unique_patients = groups.unique()
    patient_labels = df.groupby('Participant')['Remission'].first()
    n_remission_patients = sum(patient_labels == 1)
    n_non_remission_patients = sum(patient_labels == 0)
    
    # Log detailed dataset statistics
    logger.info("\nDataset Statistics:")
    logger.info(f"Total number of patients: {len(unique_patients)}")
    logger.info(f"- Remission patients: {n_remission_patients}")
    logger.info(f"- Non-remission patients: {n_non_remission_patients}")
    logger.info(f"Total number of windows: {len(df)}")
    
    # Log windows per patient statistics
    windows_per_patient = df.groupby('Participant').size()
    logger.info("\nWindows per patient:")
    logger.info(f"- Mean: {windows_per_patient.mean():.1f}")
    logger.info(f"- Min: {windows_per_patient.min()}")
    logger.info(f"- Max: {windows_per_patient.max()}")
    logger.info(f"- Median: {windows_per_patient.median()}")
    
    # Get classifiers to try
    classifiers = get_classifiers()
    
    # Initialize LOGO cross-validation
    logo = LeaveOneGroupOut()
    n_splits = logo.get_n_splits(X, y, groups)
    logger.info(f"\nPerforming Leave-One-Group-Out cross-validation with {n_splits} splits")
    
    # Dictionary to store results for each classifier
    all_patient_results = {name: [] for name in classifiers.keys()}
    all_window_results = {name: [] for name in classifiers.keys()}
    
    # Perform LOGO cross-validation
    for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):
        test_participant = groups.iloc[test_idx].unique()[0]
        true_label = y.iloc[test_idx].iloc[0]  # All windows for a patient have same label
        
        logger.info(f"\nFold {fold_idx + 1}/{n_splits}")
        logger.info(f"Testing on participant: {test_participant} (true label: {true_label})")
        
        # Split data
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        
        logger.info(f"Training windows: {len(X_train)}")
        logger.info(f"Test windows: {len(X_test)}")
        
        # Try each classifier
        for name, clf in classifiers.items():
            logger.info(f"\nTraining {name}")
            
            # Train the model
            clf.fit(X_train, y_train)
            
            # Make predictions on all windows
            window_probs = clf.predict_proba(X_test)[:, 1]
            window_preds = clf.predict(X_test)
            
            # Calculate patient-level prediction
            patient_pred, confidence = calculate_patient_prediction(window_preds)
            
            # Store window-level results
            all_window_results[name].append(pd.DataFrame({
                'participant': test_participant,
                'true_label': true_label,
                'window_prediction': window_preds,
                'window_probability': window_probs,
                'fold': fold_idx
            }))
            
            # Store patient-level results
            all_patient_results[name].append({
                'participant': test_participant,
                'true_label': true_label,
                'predicted_label': patient_pred,
                'confidence': confidence,
                'n_windows': len(test_idx),
                'n_windows_positive': sum(window_preds == 1),
                'n_windows_negative': sum(window_preds == 0),
                'proportion_positive': confidence,
                'correct_prediction': true_label == patient_pred
            })
            
            # Log fold metrics
            mlflow.log_metric(f"{name}_fold_{fold_idx}_accuracy", 
                            int(all_patient_results[name][-1]['correct_prediction']))
            mlflow.log_metric(f"{name}_fold_{fold_idx}_confidence", 
                            all_patient_results[name][-1]['confidence'])
    
    # Calculate and log overall metrics for each classifier
    best_classifier = None
    best_f1 = -1
    
    for name in classifiers.keys():
        # Combine results
        patient_results_df = pd.DataFrame(all_patient_results[name])
        window_results_df = pd.concat(all_window_results[name], ignore_index=True)
        
        # Calculate detailed metrics
        metrics = calculate_detailed_metrics(
            patient_results_df['true_label'],
            patient_results_df['predicted_label']
        )
        
        # Log detailed metrics for each classifier
        logger.info(f"\nDetailed Metrics for {name}:")
        logger.info(f"True Positives (Correct Remission): {metrics['TP']}")
        logger.info(f"True Negatives (Correct Non-Remission): {metrics['TN']}")
        logger.info(f"False Positives (Incorrect Remission): {metrics['FP']}")
        logger.info(f"False Negatives (Missed Remission): {metrics['FN']}")
        logger.info(f"Accuracy: {metrics['accuracy']:.3f}")
        logger.info(f"Precision: {metrics['precision']:.3f}")
        logger.info(f"Recall: {metrics['recall']:.3f}")
        logger.info(f"F1 Score: {metrics['f1']:.3f}")
        
        # Log metrics to MLflow
        for metric_name, value in metrics.items():
            if isinstance(value, (int, float)):  # Only log numeric metrics
                mlflow.log_metric(f"{name}_{metric_name}", value)
        
        # Track best classifier based on F1 score
        if metrics['f1'] > best_f1:
            best_f1 = metrics['f1']
            best_classifier = name
    
    logger.info(f"\nBest performing classifier: {best_classifier} (F1: {best_f1:.3f})")
    
    # Train final model on all data using best classifier
    logger.info("\nTraining final model on all data...")
    final_model = classifiers[best_classifier]
    final_model.fit(X, y)
    
    # Save results to artifacts directory
    Path(args.model_output).mkdir(parents=True, exist_ok=True)
    
    # Get results for best classifier
    patient_results_df = pd.DataFrame(all_patient_results[best_classifier])
    window_results_df = pd.concat(all_window_results[best_classifier], ignore_index=True)
    
    # Save predictions
    patient_results_df.to_csv(Path(args.model_output) / 'patient_level_predictions.csv', index=False)
    window_results_df.to_csv(Path(args.model_output) / 'window_level_predictions.csv', index=False)
    
    # Save feature importances if available
    if hasattr(final_model.named_steps['clf'], 'feature_importances_'):
        feature_importance_df = pd.DataFrame({
            'feature': X.columns,
            'importance': final_model.named_steps['clf'].feature_importances_
        }).sort_values('importance', ascending=False)
        
        # Save feature importances
        feature_importance_df.to_csv(Path(args.model_output) / 'feature_importances.csv', index=False)
        
        # Log top features
        logger.info("\nTop 10 most important features:")
        for _, row in feature_importance_df.head(10).iterrows():
            logger.info(f"{row['feature']}: {row['importance']:.4f}")
            mlflow.log_metric(f"feature_importance_{row['feature']}", row['importance'])
    
    # Save model to a separate model directory
    model_save_path = Path(args.model_output) / 'model'
    
    # Save model using sklearn format
    signature = infer_signature(X, final_model.predict_proba(X)[:, 1])
    mlflow.sklearn.save_model(
        sk_model=final_model,
        path=model_save_path,
        signature=signature
    )
    
    # Log the model in MLflow
    mlflow.sklearn.log_model(
        sk_model=final_model,
        artifact_path="model",
        registered_model_name=args.model_name,
        signature=signature
    )
    
    # Log model parameters
    for param_name, param_value in final_model.named_steps['clf'].get_params().items():
        mlflow.log_param(f"best_model_{param_name}", param_value)
    
    # Log final metrics
    logger.info("\nMisclassified Patients:")
    misclassified = patient_results_df[
        patient_results_df['true_label'] != patient_results_df['predicted_label']
    ]
    for _, row in misclassified.iterrows():
        logger.info(
            f"Participant {row['participant']}: "
            f"True={row['true_label']}, Pred={row['predicted_label']}, "
            f"Confidence={row['confidence']:.3f}, "
            f"Positive Windows: {row['n_windows_positive']}/{row['n_windows']}"
        )
    
    logger.info("Training completed successfully")

if __name__ == "__main__":
    mlflow.start_run()
    args = parse_args()
    main(args)
    mlflow.end_run()

================
File: src/multi/upsampler.py
================
import argparse
from pathlib import Path
import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
import mlflow
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("upsampler")
    parser.add_argument("--input_data", type=str, help="Path to input data directory")
    parser.add_argument("--output_data", type=str, help="Path to output data directory")
    parser.add_argument("--upsampling_factor", type=int, help="Upsampling factor")
    args = parser.parse_args()
    return args

def upsample_data(data: np.ndarray, target_length: int) -> np.ndarray:
    """Upsample data to target length using linear interpolation."""
    x = np.arange(len(data))
    interpolator = interp1d(x, data, kind='linear')
    x_new = np.linspace(0, len(data) - 1, target_length)
    return interpolator(x_new)

def main():
    mlflow.start_run()
    args = parse_args()
    
    try:
        start_time = time.time()
        input_path = Path(args.input_data)
        output_path = Path(args.output_data)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Fixed parameters
        channel_names = ['af7', 'af8', 'tp9', 'tp10']
        target_length = 2560 * args.upsampling_factor  # Upsampled length
        
        # Process each group
        for group in ['non_remission', 'remission']:
            logger.info(f"Processing {group} data...")
            
            # Load data
            input_file = input_path / f"{group}.parquet"
            df = pd.read_parquet(input_file)
            logger.info(f"Loaded {len(df)} windows for {group}")
            
            # Process each window
            processed_data = []
            for idx, row in df.iterrows():
                if idx % 100 == 0:
                    logger.info(f"Processing window {idx+1}/{len(df)}")
                
                # Initialize data dictionary
                data_dict = {'Participant': row['Participant']}
                
                # Process each channel
                for channel in channel_names:
                    channel_data = np.array(row[channel])
                    upsampled_data = upsample_data(channel_data, target_length)
                    data_dict[channel] = upsampled_data
                
                processed_data.append(data_dict)
            
            # Create DataFrame and save
            output_df = pd.DataFrame(processed_data)
            output_file = output_path / f"{group}.parquet"
            output_df.to_parquet(output_file)
            
            # Log metrics
            mlflow.log_metric(f"{group}_windows_processed", len(output_df))
            
            # Log sample lengths for verification
            for channel in channel_names:
                channel_length = len(output_df[channel].iloc[0])
                mlflow.log_metric(f"{group}_{channel}_length", channel_length)
        
        # Log execution time
        process_time = time.time() - start_time
        mlflow.log_metric("total_processing_time", process_time)
        mlflow.log_metric("upsampling_status", 1)
        
        logger.info(f"Upsampling completed in {process_time:.2f} seconds")
        
    except Exception as e:
        logger.error(f"Error in upsampling: {str(e)}")
        mlflow.log_metric("upsampling_status", 0)
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()

================
File: src/multi/window_slicer.py
================
# window_slicer.py
import argparse
from pathlib import Path
import numpy as np
import pandas as pd
import mlflow
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    parser = argparse.ArgumentParser("window_slicer")
    parser.add_argument("--input_data", type=str, help="Path to input data directory")
    parser.add_argument("--output_data", type=str, help="Path to output data directory")
    parser.add_argument("--window_seconds", type=int, default=2, help="Window length in seconds")
    parser.add_argument("--sampling_rate", type=int, default=256, help="Sampling rate in Hz")
    args = parser.parse_args()
    return args

def slice_window(data: np.ndarray, window_length: int) -> np.ndarray:
    """Slice array to specified window length."""
    if len(data) >= window_length:
        return data[:window_length]
    else:
        logger.warning(f"Data length {len(data)} shorter than window length {window_length}")
        return np.pad(data, (0, window_length - len(data)), 'constant')

def main():
    mlflow.start_run()
    args = parse_args()
    
    try:
        start_time = time.time()
        
        # Calculate desired window length
        window_length = args.window_seconds * args.sampling_rate
        logger.info(f"Window length: {window_length} points "
                   f"({args.window_seconds} seconds at {args.sampling_rate} Hz)")
        
        input_path = Path(args.input_data)
        output_path = Path(args.output_data)
        output_path.mkdir(parents=True, exist_ok=True)
        
        channels = ['af7', 'af8', 'tp9', 'tp10']
        
        # Process each group
        for group in ['non_remission', 'remission']:
            logger.info(f"Processing {group} data...")
            
            # Load data
            input_file = input_path / f"{group}.parquet"
            df = pd.read_parquet(input_file)
            logger.info(f"Loaded {len(df)} windows for {group}")
            
            # Process each window
            processed_data = []
            
            for idx, row in df.iterrows():
                if idx % 100 == 0:
                    logger.info(f"Processing window {idx+1}/{len(df)}")
                
                window_dict = {'Participant': row['Participant']}
                
                # Slice each channel
                for channel in channels:
                    channel_data = np.array(row[channel])
                    sliced_data = slice_window(channel_data, window_length)
                    window_dict[channel] = sliced_data
                    
                    # Log length check for first window
                    if idx == 0:
                        mlflow.log_metric(f"{group}_{channel}_window_length", 
                                        len(sliced_data))
                
                processed_data.append(window_dict)
            
            # Create DataFrame and save
            output_df = pd.DataFrame(processed_data)
            output_file = output_path / f"{group}.parquet"
            output_df.to_parquet(output_file)
            
            # Log metrics
            mlflow.log_metric(f"{group}_windows_processed", len(output_df))
        
        # Log execution metrics
        process_time = time.time() - start_time
        mlflow.log_metric("total_processing_time", process_time)
        mlflow.log_metric("window_slicing_status", 1)
        
        logger.info(f"Window slicing completed in {process_time:.2f} seconds")
        
    except Exception as e:
        logger.error(f"Error in window slicing: {str(e)}")
        mlflow.log_metric("window_slicing_status", 0)
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()

================
File: src/split_data_patient.py
================
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
import mlflow
from sklearn.model_selection import train_test_split
import os
import json

def parse_args():
    parser = argparse.ArgumentParser("Split data for RAI analysis")
    parser.add_argument("--input_mltable", type=str, help="Input MLTable path")
    parser.add_argument("--train_data", type=str, help="Output path for train data")
    parser.add_argument("--test_data", type=str, help="Output path for test data")
    parser.add_argument("--test_size", type=float, default=0.2, help="Test size ratio")
    parser.add_argument("--random_state", type=int, default=42, help="Random state for splitting")
    return parser.parse_args()

def read_mltable(mltable_path):
    """Read data from an MLTable directory containing a Parquet file."""
    print(f"Checking directory structure for MLTable at {mltable_path}")
    print("Contents of input MLTable directory:")
    for root, dirs, files in os.walk(mltable_path):
        print(root, "contains directories:", dirs, "and files:", files)
    
    parquet_files = list(Path(mltable_path).glob("*.parquet"))
    if not parquet_files:
        print("No Parquet found in root of MLTable directory, checking subdirectories...")
        parquet_files = list(Path(mltable_path).glob("**/*.parquet"))
    
    if not parquet_files:
        raise FileNotFoundError(f"No Parquet file found in the MLTable directory: {mltable_path}")
    
    print(f"Loading data from: {parquet_files[0]}")
    return pd.read_parquet(parquet_files[0])

def aggregate_windows_to_patient(df):
    """
    Aggregate window-level features to patient-level features.
    """
    feature_cols = df.columns.difference(['Participant', 'Remission'])
    
    # Define aggregation functions
    # Ensure zero_crossings features are treated as numeric
    agg_funcs = {col: ['mean', 'std', 'min', 'max', 'median'] 
                 for col in feature_cols}
    agg_funcs['Remission'] = 'first'
    
    # Aggregate
    patient_df = df.groupby('Participant').agg(agg_funcs)
    
    # Add percentiles
    percentiles = [25, 75]
    for col in feature_cols:
        for p in percentiles:
            patient_df[(col, f'percentile_{p}')] = df.groupby('Participant')[col].quantile(p/100)
    
    # Flatten column names
    patient_df.columns = [f"{col}_{agg}" if agg != 'first' else col 
                         for col, agg in patient_df.columns]
    
    # Add number of windows as a feature
    patient_df['n_windows'] = df.groupby('Participant').size()
    
    # Ensure all numeric columns are float64
    numeric_cols = patient_df.select_dtypes(include=['number']).columns
    patient_df[numeric_cols] = patient_df[numeric_cols].astype('float64')
    
    return patient_df.reset_index()

def main():
    mlflow.start_run()
    args = parse_args()
    
    # Read the MLTable
    df = read_mltable(args.input_mltable)
    print(f"Window-level data loaded - Shape: {df.shape}")
    
    # Aggregate to patient level first
    patient_df = aggregate_windows_to_patient(df)
    print(f"\nAggregated to patient level - Shape: {patient_df.shape}")
    print(f"Number of features per patient: {patient_df.shape[1]-2}")  # -2 for Participant and Remission
    
    # Drop Participant ID before saving (not a feature)
    train_df, test_df = train_test_split(
        patient_df.drop('Participant', axis=1),  # Remove Participant ID
        test_size=args.test_size,
        random_state=args.random_state,
        stratify=patient_df['Remission']
    )
    
    print(f"\nAfter splitting:")
    print(f"Train set shape: {train_df.shape}")
    print(f"Test set shape: {test_df.shape}")
    print(f"Train set remission patients: {train_df['Remission'].sum()}")
    print(f"Test set remission patients: {test_df['Remission'].sum()}")
    
    # Save train and test data
    train_path = Path(args.train_data)
    test_path = Path(args.test_data)
    train_path.mkdir(parents=True, exist_ok=True)
    test_path.mkdir(parents=True, exist_ok=True)
    
    # Save as CSV
    train_df.to_csv(train_path / "data.csv", index=False)
    test_df.to_csv(test_path / "data.csv", index=False)
    
    # Create MLTable definitions
    mltable_content = {
        "type": "mltable",
        "paths": [{"file": "data.csv"}],
        "transformations": [{"read_delimited": {"delimiter": ","}}]
    }
    
    with open(train_path / "MLTable", "w") as f:
        json.dump(mltable_content, f)
    with open(test_path / "MLTable", "w") as f:
        json.dump(mltable_content, f)
    
    # Log metrics
    mlflow.log_metric("total_patients", len(patient_df))
    mlflow.log_metric("train_patients", len(train_df))
    mlflow.log_metric("test_patients", len(test_df))
    mlflow.log_metric("total_features", train_df.shape[1]-1)  # -1 for Remission column
    mlflow.log_metric("train_remission_patients", train_df['Remission'].sum())
    mlflow.log_metric("test_remission_patients", test_df['Remission'].sum())
    
    # Log summary statistics across all features instead of individual feature stats
    feature_cols = [col for col in train_df.columns if col != 'Remission']
    feature_stats = train_df[feature_cols].describe()
    
    # Log average statistics across all features
    mlflow.log_metric("train_features_mean_avg", feature_stats.loc['mean'].mean())
    mlflow.log_metric("train_features_std_avg", feature_stats.loc['std'].mean())
    mlflow.log_metric("train_features_min_avg", feature_stats.loc['min'].mean())
    mlflow.log_metric("train_features_max_avg", feature_stats.loc['max'].mean())
    
    print("\nExample features:")
    print(train_df.columns[:10].tolist())
    
    mlflow.end_run()

if __name__ == "__main__":
    main()

================
File: src/split_data_window.py
================
import argparse
import pandas as pd
from pathlib import Path
import mlflow
from sklearn.model_selection import train_test_split
import os
import json

def parse_args():
    parser = argparse.ArgumentParser("Split data for RAI analysis")
    parser.add_argument("--input_mltable", type=str, help="Input MLTable path")
    parser.add_argument("--train_data", type=str, help="Output path for train data")
    parser.add_argument("--test_data", type=str, help="Output path for test data")
    parser.add_argument("--test_size", type=float, default=0.2, help="Test size ratio")
    parser.add_argument("--random_state", type=int, default=42, help="Random state for splitting")
    return parser.parse_args()

def read_mltable(mltable_path):
    """Read data from an MLTable directory containing a Parquet file."""
    print(f"Checking directory structure for MLTable at {mltable_path}")
    print("Contents of input MLTable directory:")
    for root, dirs, files in os.walk(mltable_path):
        print(root, "contains directories:", dirs, "and files:", files)

    # Look for Parquet files directly in the directory
    parquet_files = list(Path(mltable_path).glob("*.parquet"))
    
    # If no Parquet found, check subdirectories
    if not parquet_files:
        print("No Parquet found in root of MLTable directory, checking subdirectories...")
        parquet_files = list(Path(mltable_path).glob("**/*.parquet"))
    
    if not parquet_files:
        raise FileNotFoundError(f"No Parquet file found in the MLTable directory: {mltable_path}")
    
    # Load the first Parquet file found
    print(f"Loading data from: {parquet_files[0]}")
    return pd.read_parquet(parquet_files[0])


def main():
    mlflow.start_run()
    args = parse_args()
    
    # Read the MLTable
    df = read_mltable(args.input_mltable)
    
    # Confirm DataFrame loaded
    print(f"DataFrame loaded - Columns: {df.columns.tolist()}, Shape: {df.shape}")
    
    # Drop Participant column if it exists
    if 'Participant' in df.columns:
        df = df.drop(['Participant'], axis=1)
    
    # Split data
    train_df, test_df = train_test_split(
        df,
        test_size=args.test_size,
        random_state=args.random_state,
        stratify=df.get('Remission')  # Use `.get()` to avoid KeyError if missing
    )
    
    # Save train and test data as CSV
    train_path = Path(args.train_data)
    test_path = Path(args.test_data)
    train_path.mkdir(parents=True, exist_ok=True)
    test_path.mkdir(parents=True, exist_ok=True)
    
    train_df.to_csv(train_path / "data.csv", index=False)
    test_df.to_csv(test_path / "data.csv", index=False)
    
    # Create MLTable definitions
    mltable_content = {
        "type": "mltable",
        "paths": [{"file": "data.csv"}],
        "transformations": [{"read_delimited": {"delimiter": ","}}]
    }
    
    with open(train_path / "MLTable", "w") as f:
        json.dump(mltable_content, f)
    with open(test_path / "MLTable", "w") as f:
        json.dump(mltable_content, f)
    
    # Log metrics
    mlflow.log_metric("train_samples", len(train_df))
    mlflow.log_metric("test_samples", len(test_df))
    mlflow.end_run()

if __name__ == "__main__":
    main()
